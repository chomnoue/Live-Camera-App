{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model to decode sequences of digits from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "def load_data(file_name,valid_size=0):\n",
    "     with open(file_name, 'r') as f:\n",
    "        data=pickle.load(f)\n",
    "        labels=data[\"label\"]\n",
    "        images=data[\"image\"]\n",
    "        if valid_size:\n",
    "            all_data=zip(images,labels)\n",
    "            np.random.shuffle(all_data)\n",
    "            valid_data=all_data[:valid_size]\n",
    "            valid_images,valid_labels=tuple([list(l) for l in zip(*valid_data)])\n",
    "            train_data=all_data[valid_size:]\n",
    "            train_images,train_labels=tuple([list(l) for l in zip(*train_data)])\n",
    "            return train_images,train_labels,valid_images,valid_labels\n",
    "        return images,labels\n",
    "valid_size=1670 #about 5% of the data\n",
    "loaded_train_data,loaded_train_labels,loaded_valid_data,loaded_valid_labels=load_data(\"train.pickle\",valid_size)\n",
    "loaded_test_data,loaded_test_labels=load_data(\"test.pickle\")\n",
    "print(\"train:\",len(loaded_train_data),len(loaded_train_labels))\n",
    "print(\"validation:\",len(loaded_valid_data),len(loaded_valid_labels))\n",
    "print(\"test:\",len(loaded_test_data),len(loaded_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_labels=3\n",
    "num_labels=11\n",
    "distinct_labels=np.arange(num_labels).astype(np.float32) \n",
    "blanc_label=distinct_labels[0] # 0 represensts blanc label\n",
    "image_size=32\n",
    "\n",
    "def to_one_hot(label,distincts=None):\n",
    "    \"\"\"convert labels to 1-hot encoding\"\"\"\n",
    "    if distincts is None:\n",
    "        distincts=distinct_labels\n",
    "    return (distincts == label).astype(np.float32)\n",
    "\n",
    "\n",
    "def from_one_hot(label):\n",
    "    \"\"\"retrieve labels from 1-hot encoded\"\"\"    \n",
    "    return np.argmax(label)+(0 if len(label)==num_labels else 1)\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    \"\"\"padd labels, and then convert them to 1-hot\"\"\"\n",
    "    processed=[list() for i in range(max_labels)]\n",
    "    for image_label in labels :\n",
    "        for i in range(max_labels):\n",
    "            distincts=distinct_labels[1:] if i==0 else distinct_labels # first label cannot be blanc, so there are only 10 candidates for the first label\n",
    "            label=image_label[i] if i<len(image_label) else blanc_label \n",
    "            processed[i].append(to_one_hot(label,distincts))    \n",
    "    return [np.asanyarray(l,dtype=np.float32) for l in processed]        \n",
    "        \n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"convert rgb image to gray\n",
    "        see http://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python\"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"convert to numpy array and apply grayscale\"\"\"\n",
    "    np_images=np.asanyarray(images,dtype=np.float32)\n",
    "    gray_images=rgb2gray(np_images)\n",
    "    reshaped_images=gray_images.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    return reshaped_images\n",
    "    \n",
    "def preprocess_data(images,labels):\n",
    "    return preprocess_images(images),preprocess_labels(labels)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data,train_labels=preprocess_data(loaded_train_data,loaded_train_labels)\n",
    "valid_data,valid_labels=preprocess_data(loaded_valid_data,loaded_valid_labels)\n",
    "test_data,test_labels=preprocess_data(loaded_test_data,loaded_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display some labels and images to make sure that preprocessing ran fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def display_images(data,labels,loaded_data,loaded_labels,name=None,num_images=5):\n",
    "    print (\"showing some labels and images for %s\"%name)  \n",
    "    print (\"data shape: \",data.shape)\n",
    "    print(\"labels shape: \",[label.shape for label in labels])\n",
    "    total_images=data.shape[0]\n",
    "    for i in np.random.choice(total_images,num_images):\n",
    "        print([from_one_hot(label[i]) for label in labels])   \n",
    "        display_image(data[i].reshape(image_size,image_size))\n",
    "        if loaded_data!=None:\n",
    "            print(loaded_labels[i])\n",
    "            display_image(loaded_data[i])\n",
    "            \n",
    "display_images(train_data,train_labels,loaded_train_data,loaded_train_labels,\"train\")\n",
    "display_images(valid_data,valid_labels,loaded_valid_data,loaded_valid_labels,\"valid\")\n",
    "display_images(test_data,test_labels,loaded_test_data,loaded_test_labels,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IndexGenerator(object):\n",
    "    \"\"\"generates the next index of the data from wich we can take a subset of length batch_size\"\"\"\n",
    "    def __init__(self, length, batch_size):\n",
    "        self._step=0\n",
    "        self._length=length\n",
    "        self._batch_size=batch_size\n",
    "    def _next(self):\n",
    "        offset = (self._step * self._batch_size) % (self._length - self._batch_size)\n",
    "        self._step=self._step+1\n",
    "        return offset\n",
    "        \n",
    "train_batch_size=1024\n",
    "valid_batch_size=128\n",
    "test_batch_size=256\n",
    "train_index_generator=IndexGenerator(len(train_data),train_batch_size)\n",
    "valid_index_generator=IndexGenerator(len(valid_data),valid_batch_size)\n",
    "test_index_generator=IndexGenerator(len(test_data),test_batch_size)\n",
    "#print(\"train indexes:\",train_index_generator._next(),train_index_generator._next(),train_index_generator._next(),train_index_generator._next())\n",
    "#for i in range(100):\n",
    "#    print(valid_index_generator._next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \"\"\"generates a batch of data balancing zeros(blanc labels) with other labels\"\"\"\n",
    "    def __init__(self,labels,batch_size):\n",
    "        self.step=0\n",
    "        self.batch_size=batch_size\n",
    "        self.length=labels.shape[0]\n",
    "        self.with_zeros=(labels.shape[1]==num_labels)        \n",
    "        indices=np.arange(labels.shape[0])\n",
    "        self.indices=indices\n",
    "        np.random.shuffle(indices)\n",
    "        if self.with_zeros:\n",
    "            zero_indices=indices[np.argmax(labels,1)==0]\n",
    "            other_indices=indices[np.argmax(labels,1)!=0]\n",
    "            self.zero_count=zero_indices.shape[0]\n",
    "            self.other_count=other_indices.shape[0]\n",
    "            assert self.zero_count+self.other_count==labels.shape[0], \"different lengths found:%d + %d != %d\"% (self.zero_count,\n",
    "                                                                                                                self.other_count,labels.shape[0])\n",
    "            self.zero_indices=zero_indices\n",
    "            self.other_indices=other_indices\n",
    "            self.zeroes_batch_size=batch_size/num_labels\n",
    "            self.other_batch_size=batch_size-self.zeroes_batch_size\n",
    "            \n",
    "        \n",
    "    def _next(self):\n",
    "        if self.with_zeros:\n",
    "            indices=[]\n",
    "            zero_offset = (self.step * self.zeroes_batch_size) % (self.zero_count - self.zeroes_batch_size)\n",
    "            indices.extend(self.zero_indices[zero_offset:zero_offset+self.zeroes_batch_size])\n",
    "            other_offset = (self.step * self.other_batch_size) % (self.other_count - self.other_batch_size)\n",
    "            indices.extend(self.other_indices[other_offset:other_offset+self.other_batch_size])\n",
    "            assert len(indices)==self.batch_size, \"foung bad indices size: %d!=%d\"%(len(indices),self.batch_size)\n",
    "        else:\n",
    "            offset = (self.step * self.batch_size) % (self.length - self.batch_size)\n",
    "            indices=self.indices[offset:offset+self.batch_size]\n",
    "        self.step+=1        \n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "    \n",
    "train_batch_generators=[BatchGenerator(labels,train_batch_size) for labels in train_labels]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index,generator in enumerate(train_batch_generators):  \n",
    "    #generator.step=31732\n",
    "    indices=generator._next()\n",
    "    data=train_data[indices,:]\n",
    "    labels=train_labels[index][indices,:]\n",
    "    display_images(data,[labels],None,None,name=\"some images from the generated batches by generator %d\"%(index))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator=train_batch_generators[2]\n",
    "print(np.argmax(train_labels[1][generator._next(),:],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squared_image_size=image_size*image_size\n",
    "img_size=squared_image_size\n",
    "lstm_num_nodes=128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():   \n",
    "    \n",
    "    ix = tf.Variable(tf.truncated_normal([img_size, lstm_num_nodes], stddev=0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    ib = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([img_size, lstm_num_nodes], stddev=0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    fb = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    # Memory cell: input, state and bias.                     \n",
    "    cx = tf.Variable(tf.truncated_normal([img_size, lstm_num_nodes], stddev=0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    cb = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([img_size, lstm_num_nodes], stddev=0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    ob = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "   \n",
    "    #data to predict labels from\n",
    "    tf_data = tf.placeholder(tf.float32, [None, squared_image_size])\n",
    "    \n",
    "    state_shape=tf.concat(0,[tf.slice(tf.shape(tf_data),[0],[1]),[lstm_num_nodes]])\n",
    "    state = tf.zeros(state_shape)\n",
    "    output = tf.zeros(state_shape)\n",
    "    \n",
    "    tf_train_data=[]\n",
    "    tf_train_labels=[]\n",
    "    tf_predictions=[]\n",
    "    losses=[]\n",
    "    for i in range(max_labels) :\n",
    "        num_labels_i= (num_labels-1) if i==0 else num_labels # first label has only 10 candidates\n",
    "        \n",
    "        # train data for the ith prediction model\n",
    "        tf_data_i=tf.placeholder(tf.float32, [train_batch_size, squared_image_size])\n",
    "        tf_train_data.append(tf_data_i)\n",
    "        tf_labels_i=tf.placeholder(tf.float32, [train_batch_size, num_labels_i])\n",
    "        tf_train_labels.append(tf_labels_i)\n",
    "        \n",
    "        state_i = tf.zeros([train_batch_size, lstm_num_nodes])\n",
    "        output_i = tf.zeros([train_batch_size, lstm_num_nodes])\n",
    "        \n",
    "        #compute rnn for the prediction data\n",
    "        output,state=lstm_cell(tf_data,output,state)\n",
    "        \n",
    "        #pass the i th data through the previous steps rnn\n",
    "        for _ in range(i+1):\n",
    "            output_i,state_i=lstm_cell(tf_data_i,output_i,state_i)\n",
    "        \n",
    "        # i th weights and biases.\n",
    "        weights_i = tf.Variable(tf.truncated_normal([lstm_num_nodes, num_labels_i], stddev=0.1))\n",
    "        biases_i = tf.Variable(tf.ones([num_labels_i]))\n",
    "        \n",
    "        # i th digit prediction\n",
    "        prediction_i = tf.nn.softmax(tf.matmul(output, weights_i) + biases_i)\n",
    "        tf_predictions.append(prediction_i)\n",
    "        \n",
    "        # i th loss\n",
    "        logits_i = tf.matmul(output_i, weights_i) + biases_i\n",
    "        loss_i = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits_i, tf_labels_i))\n",
    "        losses.append(loss_i)\n",
    "        \n",
    "    loss=sum(losses)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    try:\n",
    "        for step in range(num_steps):\n",
    "            train_feed_dict={}\n",
    "            for i,generator in enumerate(train_batch_generators):  \n",
    "                #generator.step=31732\n",
    "                indices=generator._next()\n",
    "                data_i=train_data[indices,:]\n",
    "                train_feed_dict[tf_train_data[i]]=data_i\n",
    "                labels_i=train_labels[i][indices,:]\n",
    "                train_feed_dict[tf_train_labels[i]]=labels_i\n",
    "            session.run([optimizer],feed_dict=train_feed_dict)\n",
    "            if (step % 10 == 0):\n",
    "                train_loss=session.run([loss],feed_dict=train_feed_dict)\n",
    "                print('Loss at step %d:' % (step), train_loss)\n",
    "                \n",
    "                valid_offset=valid_index_generator._next()\n",
    "                valid_data_sample=valid_data[valid_offset:valid_offset+valid_batch_size]\n",
    "                valid_predictions=session.run(tf_predictions,{tf_data:valid_data_sample})\n",
    "                valid_labels_sample=[label_list[valid_offset:valid_offset+valid_batch_size] for label_list in valid_labels]\n",
    "                print('Validation accuracy(%): ', [accuracy(predictions, labels) for predictions, labels\n",
    "                                                   in zip(valid_predictions,valid_labels_sample)])\n",
    "    except KeyboardInterrupt:        \n",
    "        test_offset=test_index_generator._next()\n",
    "        test_data_sample=test_data[test_offset:test_offset+test_batch_size]\n",
    "        test_predictions=session.run(tf_predictions,{tf_data:test_data_sample})\n",
    "        test_labels_sample=[label_list[test_offset:test_offset+test_batch_size] for label_list in test_labels]\n",
    "        print('Test accuracy(%): ', [accuracy(predictions, labels) for predictions, labels\n",
    "                                      in zip(test_predictions,test_labels_sample)])\n",
    "        display_images(test_data_sample,test_predictions,\n",
    "                      loaded_test_data[test_offset:test_offset+test_batch_size],\n",
    "                       loaded_test_labels[test_offset:test_offset+test_batch_size],\"displaying predicted labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed=[list() for i in range(3)]\n",
    "processed[1].append(2)\n",
    "print(processed)\n",
    "i=4\n",
    "processed[i] if i<len(processed) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.contrib.losses.sigmoid_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "\n",
    "    # testin cosine distance\n",
    "    a = tf.constant([[[1.0, 0.0],[0.0,1.0]],\n",
    "                    [[0.0, 1.0],[0.0,1.0]]])\n",
    "    b = tf.constant([[[0.0, 1.0],[0.0,1.0]],\n",
    "                    [[0.0, 1.0],[0.0,1.0]]])\n",
    "    c = tf.constant([[[0.95,0.05],[0.05,0.95]],\n",
    "                    [[0.05,0.95],[0.05,0.95]]])\n",
    "\n",
    "    dist =tf.contrib.losses.cosine_distance\n",
    "    print(\"dim:\",a.get_shape())\n",
    "    print(\"aa: \",dist(a,a,2).eval())\n",
    "    print(\"ab: \",dist(a,b,2).eval())\n",
    "    print(\"ac: \",dist(a,c,2).eval())\n",
    "    print(\"bc: \",dist(b,c,2).eval())\n",
    "    print(\"cb: \",dist(c,b,2).eval())\n",
    "    \n",
    "    print(\"loga:\",tf.log(a).eval())\n",
    "\n",
    "    def reshape_and_normalize(vector):\n",
    "        reshaped=tf.reshape(vector,[1,])\n",
    "        length=vector.get_shape()[1]\n",
    "        return reshaped/tf.cast(length,tf.float32)\n",
    "\n",
    "    def reshaped_dist(pred,tar):\n",
    "        return dist(reshape_and_normalize(pred),reshape_and_normalize(tar),1)\n",
    "    state_size=tf.constant(rnn_cell.state_size)\n",
    "    print(state_size)\n",
    "    print(tf.concat(0,[tf.slice(tf.shape(a),[0],[1]),[state_size]]).eval()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(valid_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
