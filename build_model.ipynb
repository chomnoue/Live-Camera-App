{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model to decode sequences of digits from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 31732 31732\n",
      "validation: 1670 1670\n",
      "test: 13068 13068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "def load_data(file_name,valid_size=0):\n",
    "     with open(file_name, 'r') as f:\n",
    "        data=pickle.load(f)\n",
    "        labels=data[\"label\"]\n",
    "        images=data[\"image\"]\n",
    "        if valid_size:\n",
    "            all_data=zip(images,labels)\n",
    "            np.random.shuffle(all_data)\n",
    "            valid_data=all_data[:valid_size]\n",
    "            valid_images,valid_labels=tuple([list(l) for l in zip(*valid_data)])\n",
    "            train_data=all_data[valid_size:]\n",
    "            train_images,train_labels=tuple([list(l) for l in zip(*train_data)])\n",
    "            return train_images,train_labels,valid_images,valid_labels\n",
    "        return images,labels\n",
    "valid_size=1670 #about 5% of the data\n",
    "train_images,train_labels,valid_images,valid_labels=load_data(\"train.pickle\",valid_size)\n",
    "test_images,test_labels=load_data(\"test.pickle\")\n",
    "print(\"train:\",len(train_images),len(train_labels))\n",
    "print(\"validation:\",len(valid_images),len(valid_labels))\n",
    "print(\"test:\",len(test_images),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img size:  32\n",
      "labels:  [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.]\n",
      "5.0 reshaped:  [[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "printed labels:  ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', ' ']\n",
      "blanc label 11.0\n",
      "blanc label reshaped [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]] (1, 11)\n",
      "blanc label reduced : 11.0\n"
     ]
    }
   ],
   "source": [
    "img_size=32\n",
    "num_channels = 3\n",
    "distinct_labels=np.arange(1,12).astype(np.float32) # 11 reprsents no more digit\n",
    "distinct_labels_size=len(distinct_labels)\n",
    "blanc_label=distinct_labels[distinct_labels_size-1]\n",
    "\n",
    "def reshape_image(image):\n",
    "    return np.reshape(image,(-1,img_size,img_size,num_channels)).astype(np.float32)\n",
    "\n",
    "def reshape_label(label):\n",
    "    return np.reshape((distinct_labels==label),(1,distinct_labels_size)).astype(np.float32)\n",
    "\n",
    "def reduce_label(reshaped_label):\n",
    "    return np.sum(np.multiply(np.transpose(reshaped_label[0]),distinct_labels))\n",
    "\n",
    "def print_label(label):\n",
    "    if label==10.:\n",
    "        return \"0\"\n",
    "    elif label==11.0:\n",
    "        return \" \"\n",
    "    return str(int(label))\n",
    "\n",
    "print(\"img size: \",img_size)\n",
    "print(\"labels: \",distinct_labels)\n",
    "print(\"5.0 reshaped: \",reshape_label(5.0))\n",
    "print(\"printed labels: \",[print_label(i) for i in distinct_labels])\n",
    "print(\"blanc label\",blanc_label)\n",
    "reshaped_blanc_label=reshape_label(blanc_label)\n",
    "print(\"blanc label reshaped\",reshaped_blanc_label,reshaped_blanc_label.shape)\n",
    "print(\"blanc label reduced :\",reduce_label(reshaped_blanc_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanding train data\n",
      "initial size:  31732 31732\n",
      "0%.........1%.........3%.........4%.........6%.........7%.........9%.........11%.........12%.........14%.........15%.........17%.........18%.........20%.........22%.........23%.........25%.........26%.........28%.........29%.........31%.........33%.........34%.........36%.........37%.........39%.........40%.........42%.........44%.........45%.........47%.........48%.........50%.........51%.........53%.........55%.........56%.........58%.........59%.........61%.........63%.........64%.........66%.........67%.........69%.........70%.........72%.........74%.........75%.........77%.........78%.........80%.........81%.........83%.........85%.........86%.........88%.........89%.........91%.........92%.........94%.........96%.........97%.........99%....transformed size:  101331 101331\n",
      "expanding valid data\n",
      "initial size:  1670 1670\n",
      "0%.........29%.........59%.........89%...transformed size:  5328 5328\n",
      "expanding test data\n",
      "initial size:  13068 13068\n",
      "0%.........3%.........7%.........11%.........15%.........19%.........22%.........26%.........30%.........34%.........38%.........42%.........45%.........49%.........53%.........57%.........61%.........65%.........68%.........72%.........76%.........80%.........84%.........88%.........91%.........95%.........99%.transformed size:  39100 39100\n"
     ]
    }
   ],
   "source": [
    "#we can choose to expand only a subset of the data for the next steps\n",
    "train_subset = len(train_images)\n",
    "valid_subset=len(valid_images)\n",
    "test_subset=len(test_images)\n",
    "\n",
    "def expand_image(image,image_labels):\n",
    "    \"\"\"expand the image to have one instance for each digit\"\"\"  \n",
    "    reshaped_images = list()\n",
    "    reshaped_labels=list()\n",
    "    reshaped_image=reshape_image(image)\n",
    "    image_labels=image_labels+[blanc_label]#the last prediction should be blanc label\n",
    "    for image_label in image_labels:\n",
    "        reshaped_images.append(reshaped_image)\n",
    "        reshaped_labels.append(reshape_label(image_label))\n",
    "    return reshaped_images,reshaped_labels\n",
    "    \n",
    "def expand_data(images,labels):\n",
    "    \"\"\"expand the images to have one instance for each digit\"\"\"\n",
    "    print(\"initial size: \",len(images),len(labels))\n",
    "    expanded_images=list()\n",
    "    expanded_labels=list()\n",
    "    total_images=len(images)\n",
    "    image_index=-1\n",
    "    for image,image_labels in zip(images,labels):  \n",
    "        image_index=image_index+1\n",
    "        reshaped_images,reshaped_labels=expand_image(image,image_labels)\n",
    "        expanded_images.extend(reshaped_images)\n",
    "        expanded_labels.extend(reshaped_labels)\n",
    "        if image_index % 500 == 0:\n",
    "            percent=image_index*100/total_images\n",
    "            sys.stdout.write(\"%d%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        elif image_index % 50 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "    print(\"transformed size: \",len(expanded_images),len(expanded_labels))\n",
    "    return expanded_images,expanded_labels\n",
    "    \n",
    "print(\"expanding train data\")\n",
    "expanded_train_data,expanded_train_labels=expand_data(train_images[:train_subset],train_labels[:train_subset])\n",
    "print(\"expanding valid data\")\n",
    "expanded_valid_data,expanded_valid_labels=expand_data(valid_images[:valid_subset],valid_labels[:valid_subset])\n",
    "print(\"expanding test data\")\n",
    "expanded_test_data,expanded_test_labels=expand_data(test_images[:test_subset],test_labels[:test_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IndexGenerator(object):\n",
    "    \"\"\"generates the next index of the data from wich we can take a subset of length batch_size\"\"\"\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self._index=0\n",
    "        self._labels=labels\n",
    "        self._length=len(labels)\n",
    "        self._batch_size=batch_size\n",
    "    def _next(self):\n",
    "        curr_index=self._index\n",
    "        self._index=self._index+self._batch_size\n",
    "        #make sure we are at the starting of an image (just after a blank label)\n",
    "        while self._index < self._length and reduce_label(self._labels[self._index-1])!=blanc_label:\n",
    "            self._index=self._index+1\n",
    "        if self._index+self._batch_size > self._length:\n",
    "            self._index=0\n",
    "        #if(curr_index!=0):\n",
    "        #    print(self._labels[curr_index-1],self._labels[curr_index])\n",
    "        return curr_index\n",
    "        \n",
    "train_batch_size=128\n",
    "valid_batch_size=128\n",
    "test_batch_size=128\n",
    "train_index_generator=IndexGenerator(expanded_train_labels,train_batch_size)\n",
    "valid_index_generator=IndexGenerator(expanded_valid_labels,valid_batch_size)\n",
    "test_index_generator=IndexGenerator(expanded_test_labels,test_batch_size)\n",
    "#print(\"train indexes:\",train_index_generator._next(),train_index_generator._next(),train_index_generator._next(),train_index_generator._next())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch_size = 5\n",
    "conv1_size = 48\n",
    "conv2_size = 64\n",
    "conv3_size = 128\n",
    "\n",
    "num_nodes = 64 #lstm nodes\n",
    "num_hidden = 64 #convolution features\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #conv parameters\n",
    "    conv1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, conv1_size], stddev=0.1))\n",
    "    conv1_biases = tf.Variable(tf.zeros([conv1_size]))\n",
    "    conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, conv1_size, conv2_size], stddev=0.1))\n",
    "    conv2_biases = tf.Variable(tf.constant(1.0, shape=[conv2_size]))  \n",
    "    conv3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, conv2_size, conv3_size], stddev=0.1))\n",
    "    conv3_biases = tf.Variable(tf.zeros([conv3_size]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [img_size // 4 * img_size // 8 * num_hidden, num_hidden], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    def conv(data):\n",
    "        \"\"\"apply our three layer convolution on the image\"\"\"\n",
    "        #print(\"data shape\",data.get_shape().as_list())\n",
    "        conv1 = tf.nn.conv2d(data, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "        pool1 = tf.nn.max_pool(hidden1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + conv2_biases)\n",
    "        pool2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(pool2, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden3 = tf.nn.relu(conv3 + conv3_biases)\n",
    "        pool3 = tf.nn.max_pool(hidden3, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool3.get_shape().as_list()\n",
    "        #print(\"shape: \",shape)\n",
    "        reshape = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        return tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "        \n",
    "    \n",
    "    # Cell Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([num_hidden, num_nodes], stddev=0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    ib = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([num_hidden, num_nodes], stddev=0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    fb = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                     \n",
    "    cx = tf.Variable(tf.truncated_normal([num_hidden, num_nodes], stddev=0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    cb = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([num_hidden, num_nodes], stddev=0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    ob = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    \n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    weights = tf.Variable(tf.truncated_normal([num_nodes, distinct_labels_size], stddev=0.1))\n",
    "    biases = tf.Variable(tf.ones([distinct_labels_size]))\n",
    "        \n",
    "    # Definition of the cell computation.\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    initial_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    initial_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "    \n",
    "    tf_blanc_label=tf.constant(reshape_label(blanc_label))\n",
    "    \n",
    "    def lstm_cell_data(input_data,input_labels):\n",
    "        outputs=list()\n",
    "        output=initial_output\n",
    "        state=initial_state\n",
    "        for i,label in zip(input_data,input_labels):\n",
    "            #first apply conv2d beofre running lstm cell\n",
    "            conv_i=conv(i)\n",
    "            output, state = lstm_cell(conv_i, output, state)\n",
    "            outputs.append(output)\n",
    "            #reinitialize the state in case of end of image\n",
    "            is_blanc_label=tf.reduce_all(tf.equal(tf_blanc_label,label))\n",
    "            output=tf.cond(is_blanc_label,lambda :initial_output, lambda :output)\n",
    "            #output=tf.Print(output,[is_blanc_label,label,tf_blanc_label],message=\"Blanc\")\n",
    "            state=tf.cond(is_blanc_label,lambda :initial_state, lambda :state)\n",
    "        return outputs\n",
    "       \n",
    "    def model(data):\n",
    "        \"\"\"model the data\"\"\"\n",
    "        return tf.matmul(data,weights)+biases\n",
    "    \n",
    "    def make_place_holder_list(size,shape,name):\n",
    "        place_holder_list=list()\n",
    "        for i in range(size):\n",
    "            place_holder_list.append(tf.placeholder(tf.float32, shape=shape,name=name+\"_\"+str(i)))\n",
    "        return place_holder_list\n",
    "            \n",
    "    tf_train_data=make_place_holder_list(train_batch_size,[1,img_size,img_size,num_channels],\"tf_train_data\")\n",
    "    tf_train_labels=make_place_holder_list(train_batch_size,[1,distinct_labels_size],\"tf_train_labels\")\n",
    "    tf_valid_data=make_place_holder_list(valid_batch_size,[1,img_size,img_size,num_channels],\"tf_valid_data\")\n",
    "    tf_valid_labels=make_place_holder_list(valid_batch_size,[1,distinct_labels_size],\"tf_valid_labels\")\n",
    "    tf_test_data=make_place_holder_list(test_batch_size,[1,img_size,img_size,num_channels],\"tf_test_data\")\n",
    "    tf_test_labels=make_place_holder_list(test_batch_size,[1,distinct_labels_size],\"tf_test_labels\")\n",
    "    \n",
    "    tf_train_outputs=lstm_cell_data(tf_train_data,tf_train_labels)\n",
    "    tf_valid_outputs=lstm_cell_data(tf_valid_data,tf_valid_labels)\n",
    "    tf_test_outputs=lstm_cell_data(tf_test_data,tf_test_labels)\n",
    "    \n",
    "    # Classifier.\n",
    "    tf_used_labels=tf.concat(0,tf_train_labels)\n",
    "    logits = model(tf.concat(0,tf_train_outputs))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf_used_labels))\n",
    "        \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction=tf.nn.softmax(logits)\n",
    "    valid_prediction=tf.nn.softmax(model(tf.concat(0,tf_valid_outputs)))\n",
    "    test_prediction=tf.nn.softmax(model(tf.concat(0,tf_test_outputs)))\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.422327\n",
      "Training accuracy: 8.6%\n",
      "Validation accuracy: 18.0%\n",
      "Loss at step 10: 2.106087\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 31.2%\n",
      "testation accuracy: 33.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    predictions=np.reshape(np.array(predictions),(-1,distinct_labels_size)).astype(np.float32)\n",
    "    labels=np.reshape(np.array(labels),(-1,distinct_labels_size)).astype(np.float32)\n",
    "    #print(predictions.shape,labels.shape)\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def populate_feed_dict(feed_dict,data,labels,index,size,data_variable,labels_variable):\n",
    "    for i in range(size):\n",
    "        feed_dict[data_variable[i]] = data[index+i]\n",
    "        feed_dict[labels_variable[i]] = labels[index+i]\n",
    "    return feed_dict\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    try:\n",
    "        for step in range(num_steps):\n",
    "            train_index=train_index_generator._next()\n",
    "            feed_dict=populate_feed_dict(dict(),expanded_train_data,expanded_train_labels,train_index,\n",
    "                                         train_batch_size,tf_train_data,tf_train_labels)\n",
    "            _, l, predictions,used_labels = session.run([optimizer, loss, train_prediction,tf_used_labels],feed_dict=feed_dict)\n",
    "            if (step % 10 == 0):\n",
    "                labels=expanded_train_labels[train_index:train_index+train_batch_size]\n",
    "                print('Loss at step %d: %f' % (step, l))\n",
    "                print('Training accuracy: %.1f%%' % accuracy(\n",
    "                        predictions, labels))\n",
    "                #print('Verifying acuracy, accuracy: %.1f%%' % accuracy(\n",
    "                #        used_labels, labels))\n",
    "                # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "                # just to get that one numpy array. Note that it recomputes all its graph\n",
    "                # dependencies\n",
    "                valid_index=valid_index_generator._next()\n",
    "                feed_dict=populate_feed_dict(dict(),expanded_valid_data,expanded_valid_labels,valid_index,\n",
    "                                         valid_batch_size,tf_valid_data,tf_valid_labels)\n",
    "                valid_labels=expanded_valid_labels[valid_index:valid_index+valid_batch_size]\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                        session.run([valid_prediction],feed_dict=feed_dict),valid_labels))\n",
    "    except KeyboardInterrupt:        \n",
    "        test_index=test_index_generator._next()\n",
    "        feed_dict=populate_feed_dict(dict(),expanded_test_data,expanded_test_labels,test_index,\n",
    "                                             test_batch_size,tf_test_data,tf_test_labels)\n",
    "        test_labels=expanded_test_labels[test_index:test_index+test_batch_size]\n",
    "        print('testation accuracy: %.1f%%' % accuracy(\n",
    "                session.run([test_prediction],feed_dict=feed_dict),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.nn.conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Some tensor we want to print the value of\n",
    "a = tf.constant([1.0, 3.0])\n",
    "\n",
    "# Add print operation\n",
    "a = tf.Print(a, [a],message=\"printing a\")\n",
    "\n",
    "# Add more elements of the graph using a\n",
    "b = tf.add(a, a).eval()\n",
    "print(a.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.Print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
