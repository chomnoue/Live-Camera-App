{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model to decode sequences of digits from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%env KERAS_BACKEND=tensorflow\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "def load_data(file_name):\n",
    "     with open(file_name, 'r') as f:\n",
    "        data=pickle.load(f)\n",
    "        labels=data[\"label\"].astype(np.int32)\n",
    "        images=data[\"image\"]\n",
    "        del data\n",
    "        return images,labels\n",
    "loaded_train_data,loaded_train_labels=load_data(\"train.pickle\")\n",
    "loaded_extra_data,loaded_extra_labels=load_data(\"extra.pickle\")\n",
    "test_data,test_labels=load_data(\"test.pickle\")\n",
    "print(\"train:\",loaded_train_data.shape,loaded_train_labels.shape)\n",
    "print(\"extra:\",loaded_extra_data.shape,loaded_extra_labels.shape)\n",
    "print(\"test:\",test_data.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keep just what our memory can support\n",
    "train_size=100000\n",
    "valid_size=5000\n",
    "all_train_data=np.concatenate((loaded_train_data,loaded_extra_data))\n",
    "all_train_labels=np.concatenate((loaded_train_labels,loaded_extra_labels))\n",
    "train_data=all_train_data[:train_size]\n",
    "train_labels=all_train_labels[:train_size]\n",
    "valid_data=all_train_data[train_size:train_size+valid_size]\n",
    "valid_labels=all_train_labels[train_size:train_size+valid_size]\n",
    "del loaded_train_data\n",
    "del loaded_train_labels\n",
    "del loaded_extra_data\n",
    "del loaded_extra_labels\n",
    "del all_train_data\n",
    "del all_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"train:\",train_data.shape,train_labels.shape)\n",
    "print(\"valid:\",valid_data.shape,valid_labels.shape)\n",
    "print(\"test:\",test_data.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_labels=2 #let us start by extracting the first char\n",
    "num_labels=11\n",
    "distinct_labels=np.arange(num_labels).astype(np.float32) \n",
    "blanc_label=10 # 10 represensts blanc label\n",
    "image_size=32\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display some labels and images to make sure that preprocessing ran fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def display_images(data,labels,name=None,num_images=5):\n",
    "    print (\"showing some labels and images for %s\"%name)  \n",
    "    print (\"data shape: \",data.shape)\n",
    "    print(\"labels shape: \",labels.shape)\n",
    "    total_images=data.shape[0]\n",
    "    for i in np.random.choice(total_images,num_images):\n",
    "        print(labels[i])   \n",
    "        display_image(data[i])\n",
    "            \n",
    "display_images(train_data,train_labels,\"train\")\n",
    "display_images(valid_data,valid_labels,\"valid\")\n",
    "display_images(test_data,test_labels,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=train_data[2:3,:,:]\n",
    "y=train_labels[2:3,:]\n",
    "i = 0\n",
    "for batch,lab in datagen.flow(x,y=y, batch_size=1):\n",
    "    i += 1\n",
    "    print(lab)\n",
    "    display_image(batch[0])\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LecunLCN(X, image_shape, threshold=1e-4, radius=7, use_divisor=True):\n",
    "    \"\"\"Local Contrast Normalization\"\"\"\n",
    "    \"\"\"[http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf]\"\"\"\n",
    "\n",
    "    # Get Gaussian filter\n",
    "    filter_shape = (radius, radius, image_shape[3], 1)\n",
    "\n",
    "    #self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)\n",
    "    filters = gaussian_filter(filter_shape)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    # Compute the Guassian weighted average by means of convolution\n",
    "    convout = tf.nn.conv2d(X, filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "    # Subtractive step\n",
    "    mid = int(np.floor(filter_shape[1] / 2.))\n",
    "\n",
    "    # Make filter dimension broadcastable and subtract\n",
    "    centered_X = tf.sub(X, convout)\n",
    "\n",
    "    # Boolean marks whether or not to perform divisive step\n",
    "    if use_divisor:\n",
    "        # Note that the local variances can be computed by using the centered_X\n",
    "        # tensor. If we convolve this with the mean filter, that should give us\n",
    "        # the variance at each point. We simply take the square root to get our\n",
    "        # denominator\n",
    "\n",
    "        # Compute variances\n",
    "        sum_sqr_XX = tf.nn.conv2d(tf.square(centered_X), filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "        # Take square root to get local standard deviation\n",
    "        denom = tf.sqrt(sum_sqr_XX)\n",
    "\n",
    "        per_img_mean = tf.reduce_mean(denom)\n",
    "        divisor = tf.maximum(per_img_mean, denom)\n",
    "        # Divisise step\n",
    "        new_X = tf.truediv(centered_X, tf.maximum(divisor, threshold))\n",
    "    else:\n",
    "        new_X = centered_X\n",
    "\n",
    "    return new_X\n",
    "\n",
    "\n",
    "def gaussian_filter(kernel_shape):\n",
    "    x = np.zeros(kernel_shape, dtype = float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    \n",
    "    for kernel_idx in xrange(0, kernel_shape[2]):\n",
    "        for i in xrange(0, kernel_shape[0]):\n",
    "            for j in xrange(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gauss(i - mid, j - mid)\n",
    "    \n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "def gauss(x, y, sigma=3.0):\n",
    "    Z = 2 * np.pi * sigma ** 2\n",
    "    return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch_size = 5\n",
    "conv1_size = 48\n",
    "conv2_size = 64\n",
    "conv3_size = 128\n",
    "\n",
    "train_batch_size=64\n",
    "valid_batch_size=64\n",
    "\n",
    "num_hidden = 256 #convolution features=fully connected size\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():  \n",
    "    \n",
    "    pool_size=(2,2)\n",
    "    def init_filter(shape):\n",
    "        w = np.random.randn(*shape) / np.sqrt(np.prod(shape[:-1]) + shape[-1]*np.prod(shape[:-2] / np.prod(pool_size)))\n",
    "        return w.astype(np.float32)\n",
    "    \n",
    "    #conv parameters\n",
    "    conv1_weights = tf.Variable(init_filter(\n",
    "      [patch_size, patch_size, num_channels, conv1_size]))\n",
    "    conv1_biases = tf.Variable(np.zeros([conv1_size], dtype=np.float32))\n",
    "    conv2_weights = tf.Variable(init_filter(\n",
    "          [patch_size, patch_size, conv1_size, conv2_size]))\n",
    "    conv2_biases = tf.Variable(np.zeros([conv2_size], dtype=np.float32))  \n",
    "    conv3_weights = tf.Variable(init_filter(\n",
    "          [patch_size, patch_size, conv2_size, conv3_size]))\n",
    "    conv3_biases = tf.Variable(np.zeros([conv3_size], dtype=np.float32))\n",
    "    pool3_num_features=image_size // 4 * image_size // 16 * conv3_size    \n",
    "    layer4_weights = tf.Variable((np.random.randn(pool3_num_features, num_hidden) /\n",
    "                                  np.sqrt(pool3_num_features + num_hidden)).astype(np.float32))\n",
    "    layer4_biases = tf.Variable(np.zeros([num_hidden], dtype=np.float32))\n",
    "    \n",
    "    def conv(data,shape,keep_prob = 1.0):\n",
    "        \"\"\"apply our three layer convolution on the image\"\"\" \n",
    "        lecun=LecunLCN(data, shape)\n",
    "        conv1 = tf.nn.conv2d(lecun, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        print(\"conv1 shape: \",conv1.get_shape().as_list())\n",
    "        hidden1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "        pool1 = tf.nn.max_pool(hidden1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        print(\"pool1 shape: \",pool1.get_shape().as_list())\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + conv2_biases)\n",
    "        print(\"conv2 shape: \",conv2.get_shape().as_list())\n",
    "        pool2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        print(\"pool2 shape: \",pool2.get_shape().as_list())\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(pool2, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        print(\"conv3 shape: \",conv3.get_shape().as_list())\n",
    "        hidden3 = tf.nn.relu(conv3 + conv3_biases)\n",
    "        pool3 = tf.nn.max_pool(hidden3, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool3.get_shape().as_list()\n",
    "        print(\"pool3 shape: \",shape)\n",
    "        reshape = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        print(\"reshaped: \",reshape.get_shape().as_list())\n",
    "        hidden4 = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "        return tf.nn.dropout(hidden4, keep_prob)\n",
    "   \n",
    "    #data to predict labels from (either test or validation)\n",
    "    shape = [valid_batch_size, image_size, image_size, num_channels]\n",
    "    tf_valid_data = tf.placeholder(tf.float32, shape)\n",
    "    conv_data=conv(tf_valid_data,shape)\n",
    "    \n",
    "    train_shape=[train_batch_size, image_size, image_size, num_channels]\n",
    "    tf_train_data=tf.placeholder(tf.float32,train_shape )\n",
    "    conv_train_data=conv(tf_train_data,train_shape)\n",
    "    conv_train_data_with_dropout=conv(tf_train_data,train_shape,0.5)\n",
    "    tf_train_labels=tf.placeholder(tf.int32,[train_batch_size,train_labels.shape[1]])\n",
    "    \n",
    "    tf_predictions=[]\n",
    "    tf_train_predictions=[]\n",
    "    losses=[]\n",
    "    for i in range(max_labels) :\n",
    "        \n",
    "        # i th weights and biases.\n",
    "        weights_i = tf.Variable((np.random.randn(num_hidden,num_labels) /\n",
    "                                  np.sqrt(num_hidden + num_labels)).astype(np.float32))\n",
    "        biases_i = tf.Variable(np.zeros([num_labels], dtype=np.float32))\n",
    "        \n",
    "        def model(data):\n",
    "            return tf.matmul(data, weights_i) + biases_i\n",
    "        \n",
    "        # i th digit prediction for test and validation\n",
    "        prediction_i = tf.nn.softmax(model(conv_data))\n",
    "        tf_predictions.append(prediction_i)\n",
    "        #i th train prediction\n",
    "        train_prediction_i = tf.nn.softmax(model(conv_train_data))\n",
    "        tf_train_predictions.append(train_prediction_i) \n",
    "        \n",
    "        # i th loss\n",
    "        logits_i = model(conv_train_data_with_dropout)\n",
    "        tf_labels_i=tf_train_labels[:,i]\n",
    "        loss_i = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits_i, tf_labels_i))\n",
    "        losses.append(loss_i)\n",
    "               \n",
    "    \n",
    "    tf_train_predictions=tf.pack(tf_train_predictions)\n",
    "    tf_predictions=tf.pack(tf_predictions)\n",
    "    \n",
    "    loss=tf.reduce_mean(losses)\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir=\"/notebooks\"\n",
    "checkpoint_path = os.path.join(train_dir, \"predict_digits.ckpt\")\n",
    "steps_per_iteration=1+train_data.shape[0]/train_batch_size\n",
    "print(\"steps per iteration: \",steps_per_iteration)\n",
    "train_flow=datagen.flow(train_data,y=train_labels, batch_size=train_batch_size)\n",
    "valid_flow=datagen.flow(valid_data,y=valid_labels, batch_size=valid_batch_size)\n",
    "test_flow=datagen.flow(test_data,y=test_labels, batch_size=valid_batch_size)\n",
    "def accuracy(predictions, labels):\n",
    "    print_labels(labels,predictions)\n",
    "    return (100.0 * np.sum(np.all(np.argmax(predictions, 2).T == labels,1))\n",
    "          / predictions.shape[1])\n",
    "\n",
    "def print_labels(labels_to_print,predictions_to_print):\n",
    "    print(\"tartet: \")\n",
    "    print(labels_to_print[:5])\n",
    "    print(\"predicted: \")\n",
    "    print(np.argmax(predictions_to_print, 2).T[:5]) \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "    step=0\n",
    "    print('Initialized')\n",
    "    try:\n",
    "        while True:\n",
    "            train_data_sample,train_labels_sample=train_flow.next() \n",
    "            if train_data_sample.shape[0]==train_batch_size:#the generator sometimes delever bad batch sizes\n",
    "                train_feed_dict={tf_train_data:train_data_sample,tf_train_labels:train_labels_sample}\n",
    "                session.run([optimizer],feed_dict=train_feed_dict)\n",
    "            if (step % 20 == 0):\n",
    "                if train_data_sample.shape[0]==train_batch_size:\n",
    "                    train_loss=session.run(losses,feed_dict=train_feed_dict)\n",
    "                    print('Loss at step %d:' % (step), train_loss)\n",
    "                    train_predictions=session.run(tf_train_predictions,feed_dict=train_feed_dict)\n",
    "                    print('Train accuracy(%): ', accuracy(train_predictions, train_labels_sample[:,:max_labels]))\n",
    "\n",
    "                valid_data_sample,valid_labels_sample=valid_flow.next()\n",
    "                if valid_data_sample.shape[0]==valid_batch_size:\n",
    "                    valid_predictions=session.run(tf_predictions,{tf_valid_data:valid_data_sample})\n",
    "                    print('Validation accuracy(%): ', accuracy(valid_predictions,valid_labels_sample[:,:max_labels]))\n",
    "                \n",
    "                test_data_sample,test_labels_sample=test_flow.next()\n",
    "                if test_data_sample.shape[0]==valid_batch_size:\n",
    "                    test_predictions=session.run(tf_predictions,{tf_valid_data:test_data_sample})\n",
    "                    print('Test accuracy(%): ', accuracy(test_predictions,test_labels_sample[:,:max_labels]))\n",
    "            if(step % steps_per_iteration==0):\n",
    "                print(\"saving the variables after %d iterations\" % (step/steps_per_iteration))\n",
    "                saver.save(session, checkpoint_path, global_step=step)                \n",
    "            step+=1\n",
    "    except KeyboardInterrupt:        \n",
    "        print(\"keyboard interrupt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
