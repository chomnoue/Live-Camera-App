{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model to decode sequences of digits from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "def load_data(file_name,valid_ratio=0):\n",
    "     with open(file_name, 'r') as f:\n",
    "        data=pickle.load(f)\n",
    "        labels=data[\"label\"]\n",
    "        images=data[\"image\"]\n",
    "        if valid_ratio>0:\n",
    "            valid_size=int(len(labels)*valid_ratio)\n",
    "            all_data=zip(images,labels)\n",
    "            np.random.shuffle(all_data)\n",
    "            valid_data=all_data[:valid_size]\n",
    "            valid_images,valid_labels=tuple([list(l) for l in zip(*valid_data)])\n",
    "            train_data=all_data[valid_size:]\n",
    "            train_images,train_labels=tuple([list(l) for l in zip(*train_data)])\n",
    "            return train_images,train_labels,valid_images,valid_labels\n",
    "        return images,labels\n",
    "valid_ratio=.05 #about 5% of the data\n",
    "loaded_train_data,loaded_train_labels,loaded_valid_data,loaded_valid_labels=load_data(\"train.pickle\",valid_ratio)\n",
    "loaded_test_data,loaded_test_labels=load_data(\"test.pickle\")\n",
    "print(\"train:\",len(loaded_train_data),len(loaded_train_labels))\n",
    "print(\"validation:\",len(loaded_valid_data),len(loaded_valid_labels))\n",
    "print(\"test:\",len(loaded_test_data),len(loaded_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keep just what our memory can support\n",
    "loaded_train_data=loaded_train_data[:100000]\n",
    "loaded_train_labels=loaded_train_labels[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_labels=2 #let us start by extracting the first char\n",
    "num_labels=11\n",
    "distinct_labels=np.arange(num_labels).astype(np.float32) \n",
    "blanc_label=distinct_labels[-1] # 10 represensts blanc label\n",
    "image_size=32\n",
    "num_channels = 3\n",
    "\n",
    "def to_one_hot(label,distincts=None):\n",
    "    \"\"\"convert labels to 1-hot encoding\"\"\"\n",
    "    if distincts is None:\n",
    "        distincts=distinct_labels\n",
    "    return (distincts == label).astype(np.float32)\n",
    "\n",
    "    \n",
    "def rgb2gray(rgb):\n",
    "    \"\"\"convert rgb image to gray\n",
    "        see http://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python\"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def from_one_hot(label):\n",
    "    \"\"\"retrieve labels from 1-hot encoded\"\"\"    \n",
    "    return np.argmax(label)\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    \"\"\"padd labels, and then convert them to 1-hot\"\"\"\n",
    "    processed=[list() for i in range(max_labels)]\n",
    "    for image_label in labels :\n",
    "        for i in range(max_labels):\n",
    "            if i<len(image_label):\n",
    "                label=image_label[i]\n",
    "                label=label if label!=blanc_label else 0 #replace 10 by 0\n",
    "            else:\n",
    "                label= blanc_label \n",
    "            distincts=distinct_labels[:-1] if i==0 else distinct_labels # first label cannot be blanc, so there are only 10 candidates for the first label\n",
    "            processed[i].append(to_one_hot(label,distincts))    \n",
    "    return [np.asanyarray(l,dtype=np.float32) for l in processed]        \n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"convert to numpy array and apply grayscale\"\"\"\n",
    "    np_images=np.asanyarray(images,dtype=np.float32)\n",
    "    #gray_images=rgb2gray(np_images)\n",
    "    reshaped_images=np_images.reshape((-1, image_size , image_size,num_channels)).astype(np.float32)\n",
    "    return reshaped_images\n",
    "    \n",
    "def preprocess_data(images,labels):\n",
    "    return preprocess_images(images),preprocess_labels(labels)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,train_labels=preprocess_data(loaded_train_data,loaded_train_labels)\n",
    "valid_data,valid_labels=preprocess_data(loaded_valid_data,loaded_valid_labels)\n",
    "test_data,test_labels=preprocess_data(loaded_test_data,loaded_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display some labels and images to make sure that preprocessing ran fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def display_images(data,labels,name=None,num_images=5):\n",
    "    print (\"showing some labels and images for %s\"%name)  \n",
    "    print (\"data shape: \",data.shape)\n",
    "    print(\"labels shape: \",[label.shape for label in labels])\n",
    "    total_images=data.shape[0]\n",
    "    for i in np.random.choice(total_images,num_images):\n",
    "        print([from_one_hot(label[i]) for label in labels])   \n",
    "        display_image(data[i])\n",
    "            \n",
    "display_images(train_data,train_labels,loaded_train_data,loaded_train_labels,\"train\")\n",
    "display_images(valid_data,valid_labels,loaded_valid_data,loaded_valid_labels,\"valid\")\n",
    "display_images(test_data,test_labels,loaded_test_data,loaded_test_labels,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IndexGenerator(object):\n",
    "    \"\"\"generates the next index of the data from wich we can take a subset of length batch_size\"\"\"\n",
    "    def __init__(self, length):\n",
    "        self._step=0\n",
    "        self._length=length\n",
    "    def _next(self,batch_size):\n",
    "        offset = (self._step * batch_size) % (self._length - batch_size)\n",
    "        self._step=self._step+1\n",
    "        return offset\n",
    "\n",
    "train_index_generator=IndexGenerator(len(train_data))\n",
    "valid_index_generator=IndexGenerator(len(valid_data))\n",
    "test_index_generator=IndexGenerator(len(test_data))\n",
    "#print(\"train indexes:\",train_index_generator._next(),train_index_generator._next(),train_index_generator._next(),train_index_generator._next())\n",
    "#for i in range(100):\n",
    "#    print(valid_index_generator._next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \"\"\"generates a batch of data balancing labels\"\"\"\n",
    "    def __init__(self,labels):\n",
    "        self.step=0\n",
    "        labs=np.argmax(labels,1)\n",
    "        self.indices=[np.where(labs==lab)[0] for lab in np.unique(labs)]\n",
    "        for index_set in self.indices:\n",
    "            np.random.shuffle(index_set)            \n",
    "        \n",
    "    def _next(self,batch_size):\n",
    "        indices=[]\n",
    "        batch=batch_size/len(self.indices)\n",
    "        for index_set in self.indices:\n",
    "            length=len(index_set)\n",
    "            offset = (self.step * batch) % (length - batch)\n",
    "            indices.extend(index_set[offset:offset+batch])            \n",
    "        self.step+=1        \n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "        \n",
    "        \n",
    "    \n",
    "train_batch_generators=[BatchGenerator(labels) for labels in train_labels]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index,generator in enumerate(train_batch_generators):  \n",
    "    #generator.step=31732\n",
    "    indices=generator._next(128)\n",
    "    data=train_data[indices,:]\n",
    "    labels=train_labels[index][indices,:]\n",
    "    display_images(data,[labels],None,None,name=\"some images from the generated batches by generator %d\"%(index))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator=train_batch_generators[0]\n",
    "generated_labels=np.argmax(train_labels[0][generator._next(128),:],1)\n",
    "counts={}\n",
    "for label in generated_labels:\n",
    "    if label not in counts:\n",
    "        counts[label]=0\n",
    "    counts[label]=counts[label]+1\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch_size = 5\n",
    "conv1_size = 48\n",
    "conv2_size = 64\n",
    "conv3_size = 128\n",
    "\n",
    "squared_image_size=image_size*image_size\n",
    "img_size=squared_image_size\n",
    "lstm_num_nodes=128\n",
    "num_hidden = 64 #convolution features\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():  \n",
    "    \n",
    "    #conv parameters\n",
    "    conv1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, conv1_size], stddev=0.1))\n",
    "    conv1_biases = tf.Variable(tf.zeros([conv1_size]))\n",
    "    conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, conv1_size, conv2_size], stddev=0.1))\n",
    "    conv2_biases = tf.Variable(tf.constant(1.0, shape=[conv2_size]))  \n",
    "    conv3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, conv2_size, conv3_size], stddev=0.1))\n",
    "    conv3_biases = tf.Variable(tf.zeros([conv3_size]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [image_size // 4 * image_size // 16 * conv3_size, num_hidden], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    def conv(data):\n",
    "        \"\"\"apply our three layer convolution on the image\"\"\"        \n",
    "        conv1 = tf.nn.conv2d(data, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "        pool1 = tf.nn.max_pool(hidden1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + conv2_biases)\n",
    "        pool2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv3 = tf.nn.conv2d(pool2, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden3 = tf.nn.relu(conv3 + conv3_biases)\n",
    "        pool3 = tf.nn.max_pool(hidden3, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool3.get_shape().as_list()\n",
    "        #print(\"shape: \",shape)\n",
    "        reshape = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        #print(\"reshaped: \",reshape.get_shape().as_list())\n",
    "        return tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    \n",
    "    ix = tf.Variable(tf.truncated_normal([num_hidden, lstm_num_nodes], stddev=0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    ib = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([num_hidden, lstm_num_nodes], stddev=0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    fb = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    # Memory cell: input, state and bias.                     \n",
    "    cx = tf.Variable(tf.truncated_normal([num_hidden, lstm_num_nodes], stddev=0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    cb = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([num_hidden, lstm_num_nodes], stddev=0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes], stddev=0.1))\n",
    "    ob = tf.Variable(tf.ones([1, lstm_num_nodes]))\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "   \n",
    "    #data to predict labels from\n",
    "    tf_data = tf.placeholder(tf.float32, [None, image_size, image_size, num_channels])\n",
    "    conv_data=conv(tf_data)\n",
    "    \n",
    "    state_shape=tf.concat(0,[tf.slice(tf.shape(tf_data),[0],[1]),[lstm_num_nodes]])\n",
    "    state = tf.zeros(state_shape)\n",
    "    output = tf.zeros(state_shape)\n",
    "    \n",
    "    tf_train_data=[]\n",
    "    tf_train_labels=[]\n",
    "    tf_predictions=[]\n",
    "    losses=[]\n",
    "    for i in range(max_labels) :\n",
    "        num_labels_i= (num_labels-1) if i==0 else num_labels # first label has only 10 candidates\n",
    "        \n",
    "        # train data for the ith prediction model\n",
    "        tf_data_i=tf.placeholder(tf.float32, [None, image_size, image_size, num_channels])\n",
    "        tf_train_data.append(tf_data_i)\n",
    "        conv_data_i=conv(tf_data_i)\n",
    "        tf_labels_i=tf.placeholder(tf.float32, [None, num_labels_i])\n",
    "        tf_train_labels.append(tf_labels_i)\n",
    "        \n",
    "        state_i_shape=tf.concat(0,[tf.slice(tf.shape(tf_data_i),[0],[1]),[lstm_num_nodes]])\n",
    "        state_i = tf.zeros(state_i_shape)\n",
    "        output_i = tf.zeros(state_i_shape)\n",
    "        \n",
    "        #compute rnn for the prediction data\n",
    "        output,state=lstm_cell(conv_data,output,state)\n",
    "        \n",
    "        #pass the i th data through the previous steps rnn\n",
    "        for _ in range(i+1):\n",
    "            output_i,state_i=lstm_cell(conv_data_i,output_i,state_i)\n",
    "        \n",
    "        # i th weights and biases.\n",
    "        weights_i = tf.Variable(tf.truncated_normal([lstm_num_nodes, num_labels_i], stddev=0.1))\n",
    "        biases_i = tf.Variable(tf.ones([num_labels_i]))\n",
    "        \n",
    "        def model(data):\n",
    "            hidden=tf.nn.relu(data)\n",
    "            return tf.matmul(data, weights_i) + biases_i\n",
    "        \n",
    "        # i th digit prediction\n",
    "        prediction_i = tf.nn.softmax(model(output))\n",
    "        tf_predictions.append(prediction_i)\n",
    "        \n",
    "        # i th loss\n",
    "        logits_i = model(output_i)\n",
    "        loss_i = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits_i, tf_labels_i))\n",
    "        losses.append(loss_i)\n",
    "        \n",
    "    loss=sum(losses)\n",
    "    optimizer = tf.train.AdamOptimizer(0.5).minimize(loss)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch_size=128\n",
    "valid_batch_size=64\n",
    "test_batch_size=128\n",
    "train_dir=\"/notebooks\"\n",
    "checkpoint_path = os.path.join(train_dir, \"predict_digits.ckpt\")\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def print_labels(labels_to_print,predictions_to_print):\n",
    "    print(\"tartet: \")\n",
    "    print([np.argmax(label,1)[:5] for label in labels_to_print])\n",
    "    print(\"predicted: \")\n",
    "    print([np.argmax(label,1)[:5] for label in predictions_to_print]) \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    try:\n",
    "        step=0\n",
    "        while True:\n",
    "            train_feed_dict={}\n",
    "            for i,generator in enumerate(train_batch_generators):\n",
    "                indices=generator._next(train_batch_size)\n",
    "                data_i=train_data[indices,:]\n",
    "                train_feed_dict[tf_train_data[i]]=data_i\n",
    "                labels_i=train_labels[i][indices,:]\n",
    "                train_feed_dict[tf_train_labels[i]]=labels_i\n",
    "            session.run([optimizer],feed_dict=train_feed_dict)\n",
    "            if (step % 20 == 0):\n",
    "                train_loss=session.run(losses,feed_dict=train_feed_dict)\n",
    "                print('Loss at step %d:' % (step), train_loss)\n",
    "                \n",
    "                valid_offset=valid_index_generator._next(valid_batch_size)\n",
    "                valid_data_sample=valid_data[valid_offset:valid_offset+valid_batch_size]\n",
    "                valid_predictions=session.run(tf_predictions,{tf_data:valid_data_sample})\n",
    "                valid_labels_sample=[label_list[valid_offset:valid_offset+valid_batch_size] for label_list in valid_labels]\n",
    "                print('Validation accuracy(%): ', [accuracy(predictions, labels) for predictions, labels\n",
    "                                                   in zip(valid_predictions,valid_labels_sample)])\n",
    "            if (step % 60 == 0):\n",
    "                print(\"showing some predictions\")\n",
    "                print_labels(valid_labels_sample,valid_predictions)\n",
    "            if(step % 1000==0):\n",
    "                print(\"saving the variables\")\n",
    "                saver.save(session, checkpoint_path, global_step=step)\n",
    "            step+=1\n",
    "    except KeyboardInterrupt:        \n",
    "        test_offset=test_index_generator._next(test_batch_size)\n",
    "        test_data_sample=test_data[test_offset:test_offset+test_batch_size]\n",
    "        test_predictions=session.run(tf_predictions,{tf_data:test_data_sample})\n",
    "        test_labels_sample=[label_list[test_offset:test_offset+test_batch_size] for label_list in test_labels]\n",
    "        print('Test accuracy(%): ', [accuracy(predictions, labels) for predictions, labels\n",
    "                                      in zip(test_predictions,test_labels_sample)])\n",
    "        print(\"showing some test predictions\")\n",
    "        print_labels(test_labels_sample,test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
