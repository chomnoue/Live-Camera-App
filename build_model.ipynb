{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model to decode sequences of digits from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 31732 31732\n",
      "validation: 1670 1670\n",
      "test: 13068 13068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "def load_data(file_name,valid_size=0):\n",
    "     with open(file_name, 'r') as f:\n",
    "        data=pickle.load(f)\n",
    "        labels=data[\"label\"]\n",
    "        images=data[\"image\"]\n",
    "        if valid_size:\n",
    "            all_data=zip(images,labels)\n",
    "            np.random.shuffle(all_data)\n",
    "            valid_data=all_data[:valid_size]\n",
    "            valid_images,valid_labels=tuple([list(l) for l in zip(*valid_data)])\n",
    "            train_data=all_data[valid_size:]\n",
    "            train_images,train_labels=tuple([list(l) for l in zip(*train_data)])\n",
    "            return train_images,train_labels,valid_images,valid_labels\n",
    "        return images,labels\n",
    "valid_size=1670 #about 5% of the data\n",
    "train_images,train_labels,valid_images,valid_labels=load_data(\"train.pickle\",valid_size)\n",
    "test_images,test_labels=load_data(\"test.pickle\")\n",
    "print(\"train:\",len(train_images),len(train_labels))\n",
    "print(\"validation:\",len(valid_images),len(valid_labels))\n",
    "print(\"test:\",len(test_images),len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', ' ']\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "img_size=32*32*3\n",
    "distinct_labels=np.arange(1,12).astype(np.float32) # 11 reprsents no more digit\n",
    "distinct_labels_size=len(distinct_labels)\n",
    "blanc_label=distinct_labels[distinct_labels_size-1]\n",
    "\n",
    "def reshape_image(image):\n",
    "    return np.reshape(image,img_size).astype(np.float32)\n",
    "\n",
    "def reshape_label(label):\n",
    "    return (distinc_labels==label).astype(np.float32)\n",
    "\n",
    "def print_label(label):\n",
    "    if label==10.:\n",
    "        return \"0\"\n",
    "    elif label==11.0:\n",
    "        return \" \"\n",
    "    return str(int(label))\n",
    "\n",
    "print(distinct_labels)\n",
    "print(reshape_label(5.0))\n",
    "print([print_label(i) for i in distinct_labels])\n",
    "print(blanc_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "train_subset = 5000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([img_size, num_nodes], stddev=0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    ib = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([img_size, num_nodes], stddev=0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    fb = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                     \n",
    "    cx = tf.Variable(tf.truncated_normal([img_size, num_nodes], stddev=0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    cb = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([img_size, num_nodes], stddev=0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.1))\n",
    "    ob = tf.Variable(tf.ones([1, num_nodes]))\n",
    "    \n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    weights = tf.Variable(tf.truncated_normal([num_nodes, distinct_labels_size], stddev=0.1))\n",
    "    biases = tf.Variable(tf.ones([distinct_labels_size]))\n",
    "        \n",
    "    # Definition of the cell computation.\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def lstm_cell_image(image,labels):\n",
    "        \"\"\"transfom image and correspondig labels by applying lstm_cell\"\"\"        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "        outputs = list()\n",
    "        output_labels=list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        i=tf.constant(reshape_image(image))\n",
    "        labels=labels+[blanc_label]#the last prediction should be blanc label\n",
    "        for label in labels:\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)\n",
    "            output_labels.append(label)\n",
    "        #state saving\n",
    "        dependencies=[saved_output.assign(output), saved_state.assign(state)]\n",
    "        return outputs,output_labels,dependencies\n",
    "    \n",
    "    def lstm_cell_data(images,labels):\n",
    "        \"\"\"transfom a dataset by applying lstm_cell\"\"\"\n",
    "        outputs=list()\n",
    "        output_labels=list()\n",
    "        dependencies=list()\n",
    "        for image,image_labels in zip(images,labels):\n",
    "            image_outputs,image_output_labels, image_dependencies=lstm_cell_image(image,labels)\n",
    "            outputs.extend(image_outputs)\n",
    "            output_labels.extend(image_output_labels)\n",
    "            dependencies.extend(image_dependencies)\n",
    "        return outputs,tf.constant(output_labels),dependencies\n",
    "    \n",
    "    train_data,train_digits,train_dependencies=lstm_cell_data(train_images[:train_subset],train_labels[:train_subset])\n",
    "    valid_data,valid_digits,valid_dependencies=lstm_cell_data(valid_images,valid_labels)\n",
    "    test_data,test_digits,test_dependencies=lstm_cell(test_images,test_labels)\n",
    "    \n",
    "    def model(data):\n",
    "        \"\"\"model the data\"\"\"\n",
    "        return tf.matmul(data,weights)+biases\n",
    "    \n",
    "    \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies(train_dependencies):\n",
    "        # Classifier.\n",
    "        logits = model(train_data)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, train_digits))\n",
    "        \n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    \n",
    "    # Predictions.\n",
    "    with tf.control_dependencies(valid_dependencies):\n",
    "        valid_prediction=tf.nn.softmax(model(valid_data))\n",
    "    with tf.control_dependencies(test_dependencies):\n",
    "        test_prediction=tf.nn.softmax(model(test_data))\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 5]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "    _, l, predictions,labels = session.run([optimizer, loss, train_prediction,train_digits])\n",
    "    if (step % 100 == 0):\n",
    "        print('Loss at step %d: %f' % (step, l))\n",
    "        print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_digits))\n",
    "        # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "        # just to get that one numpy array. Note that it recomputes all its graph\n",
    "        # dependencies\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(*\n",
    "                                                       session.run([valid_prediction,valid_digits])))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(*session.run([test_prediction,test_digits])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
